\documentclass{article}
\usepackage{amsmath}
\title{Predicting the performance of co-located Applications}

\date{\today}
\begin{document}

\maketitle

Given a set of applications $1, 2, \ldots, m$, our goal is to determine which
applications co-locate well together. To this end, we want to determine the features
most useful in predicting this behavior.

Assume that for each application $i$, we are given a feature vector $x_i$ that
contains a comprehensive set of \texttt{nvprof} metrics, measured when application
$i$ is run on the target GPU architecture \emph{in isolation}.

Now, we want to determine what subset of these features are actually relevant
to predicting the performance of co-located applications. For now, we only
look at the performance of pairs of co-located applications. We can extend this
to larger co-located sets in the future (even though most GPUs might only have
enough GPU memory for two applications).

Now, we use a linear regressor of the form $x_i W x_j^T$ to model $p_{ij}$
(or the performance of applications $i$ and $j$ co-located). Here, $W$ would be
a $n \times n$ matrix, where $n$ is the total number of features in $x_i$ and $x_j$.

Training this linear regressor would give us a weight matrix $W$. Here, $|W_ij|$ being
high gives us a strong signal that the interaction of features $i$ and $j$ is important
to predicting whether $i$ and $j$ co-locate well together (assuming that all
input features are normalized to have the same range).

We could then use the top-$k$ features obtained from the above procedure as the
heavy-hitter features that can be used for downstream prediction.

The point of pruning is two-fold: it helps reduce the cost of
predicting the performance of co-located applications (mainly the cost of
profiling to generate the input feature vectors). It would also help us gain
intuition into why certain pairs of applications perform better than others.

\end{document}
