{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of simulation of different fairness policies\n",
    "\n",
    "These experiments use accumulated deficits to try to ensure that applications always receive their computed allocation of GPU time, even in the event of new jobs coming in and old jobs finishing, by keeping track of the difference between the GPU time the application should have received, and the GPU time the application actually received. Allocation of jobs to GPUs is performed in a round-based fashion, with GPUs instructed to run jobs for a fixed interval of time on all GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for plotting.\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "rc('text',\n",
    "   usetex=True)\n",
    "sns.set_style('ticks')\n",
    "font = {\n",
    "    'font.family':'Times New Roman',\n",
    "    'font.weight': 200,\n",
    "    'font.size': 10,\n",
    "}\n",
    "sns.set_style(font)\n",
    "flatui = ['#002A5E', '#FD151B', '#8EBA42', '#348ABD', '#988ED5', '#BDB76B', '#8EBA42', '#FFB5B8']\n",
    "sns.set_palette(flatui)\n",
    "paper_rc = {\n",
    "    'lines.linewidth': 2,\n",
    "    'lines.markersize': 10,\n",
    "    'mathtext.fontset': 'custom',\n",
    "    'mathtext.rm': 'Times New Roman',\n",
    "    'mathtext.bf': 'Times New Roman:bold',\n",
    "}\n",
    "sns.set_context(\"paper\", font_scale=2,  rc=paper_rc)\n",
    "current_palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports.\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get logfile paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logfile_paths_helper(directory_name):\n",
    "    logfile_paths = []\n",
    "    for root, _, file_names in os.walk(directory_name):\n",
    "        if len(file_names) > 0:\n",
    "            logfile_paths.extend(\n",
    "                [os.path.join(root, file_name)\n",
    "                 for file_name in file_names])\n",
    "    return logfile_paths\n",
    "\n",
    "def get_logfile_paths(directory_name):\n",
    "    logfile_paths = []\n",
    "    for logfile_path in get_logfile_paths_helper(directory_name):\n",
    "        m = re.match(\n",
    "            r'.*v100=(\\d+)\\.p100=(\\d+)\\.k80=(\\d+)/(.*)/seed=(\\d+)/'\n",
    "             'lambda=(\\d+\\.\\d+)\\.log', logfile_path)\n",
    "        v100s = int(m.group(1))\n",
    "        p100s = int(m.group(2))\n",
    "        k80s = int(m.group(3))\n",
    "        policy = m.group(4)\n",
    "        seed = int(m.group(5))\n",
    "        l = float(m.group(6))\n",
    "        logfile_paths.append((v100s, p100s, k80s, policy, seed,\n",
    "                              l, logfile_path))\n",
    "    return logfile_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [\"fifo\",\n",
    "            \"fifo_perf\",\n",
    "            \"fifo_packed\",\n",
    "            \"max_min_fairness\",\n",
    "            \"max_min_fairness_perf\",\n",
    "            \"max_min_fairness_packed\"]\n",
    "logfile_paths = sorted(get_logfile_paths(\n",
    "    \"/lfs/1/deepak/gpusched/scheduler/logs/multigpu_support_multigpu/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(logfile_paths, v100s, p100s, k80s, policy, seed=None):\n",
    "    if seed is None:\n",
    "        return sorted([(x[5], x[6], x[4]) for x in logfile_paths\n",
    "                       if x[0] == v100s and x[1] == p100s and\n",
    "                       x[2] == k80s and x[3] == policy])\n",
    "    else:\n",
    "        return sorted([(x[5], x[6]) for x in logfile_paths\n",
    "                       if x[0] == v100s and x[1] == p100s and\n",
    "                       x[2] == k80s and x[3] == policy and\n",
    "                       x[4] == seed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\"fifo\": \"FIFO\",\n",
    "          \"fifo_perf\": \"FIFO+perf\",\n",
    "          \"fifo_packed\": \"FIFO+perf+packed\",\n",
    "          \"max_min_fairness\": \"MMF\",\n",
    "          \"max_min_fairness_perf\": \"MMF+perf\",\n",
    "          \"max_min_fairness_packed\": \"MMF+perf+packed\"}\n",
    "def plot_metric_vs_inverse_lambda(v100s, p100s, k80s,\n",
    "                                  policies, metric_fn,\n",
    "                                  metric_label,\n",
    "                                  xmax=None,\n",
    "                                  ymax=None,\n",
    "                                  output_filename=None):\n",
    "    plt.figure(figsize=(8, 3.5))\n",
    "    ax = plt.subplot2grid((1, 1), (0, 0), colspan=1)\n",
    "\n",
    "    data = {\"input_job_rate\": [], \"metric\": [], \"seed\": [],\n",
    "            \"policy\": []}\n",
    "    for policy in policies:\n",
    "        relevant_logfile_paths = list(reversed(prune(\n",
    "            logfile_paths, v100s, p100s, k80s, policy)))\n",
    "        lambdas = [x[0] for x in relevant_logfile_paths]\n",
    "        input_job_rates = [3600.0 / x for x in lambdas]\n",
    "        metrics = [metric_fn(x[1]) for x in relevant_logfile_paths]\n",
    "        seeds = [x[2] for x in relevant_logfile_paths]\n",
    "        policies = [labels[policy] for i in range(len(metrics))]\n",
    "\n",
    "        import pandas as pd\n",
    "        data[\"input_job_rate\"] += input_job_rates\n",
    "        data[\"metric\"] += metrics\n",
    "        data[\"seed\"] += seeds\n",
    "        data[\"policy\"] += policies\n",
    "\n",
    "    sns.lineplot(x='input_job_rate', y='metric', style='policy',\n",
    "                 hue='policy',\n",
    "                 data=data, ci='sd',\n",
    "                 markers=True)\n",
    "\n",
    "    ax.set_xlabel(\"Input job rate (jobs/hr)\")\n",
    "    ax.set_ylabel(metric_label)\n",
    "    ax.set_xlim([0, xmax])\n",
    "    ax.set_ylim([0, ymax])\n",
    "    sns.despine()\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    if output_filename is not None:\n",
    "        with PdfPages(output_filename) as pdf:\n",
    "            pdf.savefig(bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot average JCT versus input job rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_jct_fn(logfile_path, min_job_id=None, max_job_id=None):\n",
    "    job_completion_times = []\n",
    "    with open(logfile_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-10000:]:\n",
    "            m = re.match(r'Job (\\d+): (\\d+\\.\\d+)', line)\n",
    "            if m is not None:\n",
    "                job_id = int(m.group(1))\n",
    "                job_completion_time = float(m.group(2))\n",
    "                if min_job_id is None or min_job_id <= job_id:\n",
    "                    if max_job_id is None or job_id <= max_job_id:\n",
    "                        job_completion_times.append(\n",
    "                            job_completion_time)\n",
    "    if len(job_completion_times) == 0:\n",
    "        return None\n",
    "    return np.mean(job_completion_times) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"V100s=25, P100s=0, K80s=0; Jobs 4000-5000\")\n",
    "plot_metric_vs_inverse_lambda(\n",
    "    v100s=25, p100s=0, k80s=0,\n",
    "    policies=policies,\n",
    "    metric_fn=lambda x: average_jct_fn(x, min_job_id=4000, max_job_id=5000),\n",
    "    metric_label=\"Average JCT\\n(hours)\",\n",
    "    ymax=500.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V100s=12, P100s=12, K80s=0; Jobs 4000-5000\")\n",
    "plot_metric_vs_inverse_lambda(\n",
    "    v100s=12, p100s=12, k80s=0,\n",
    "    policies=policies,\n",
    "    metric_fn=lambda x: average_jct_fn(x, min_job_id=4000, max_job_id=5000),\n",
    "    metric_label=\"Average JCT\\n(hours)\",\n",
    "    ymax=500.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V100s=16, P100s=8, K80s=0; Jobs 4000-5000\")\n",
    "plot_metric_vs_inverse_lambda(\n",
    "    v100s=16, p100s=8, k80s=0,\n",
    "    policies=policies,\n",
    "    metric_fn=lambda x: average_jct_fn(x, min_job_id=4000, max_job_id=5000),\n",
    "    metric_label=\"Average JCT\\n(hours)\",\n",
    "    ymax=500.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V100s=8, P100s=8, K80s=8; Jobs 4000-5000\")\n",
    "plot_metric_vs_inverse_lambda(\n",
    "    v100s=8, p100s=8, k80s=8,\n",
    "    policies=policies,\n",
    "    metric_fn=lambda x: average_jct_fn(x, min_job_id=4000, max_job_id=5000),\n",
    "    metric_label=\"Average JCT\\n(hours)\",\n",
    "    ymax=500.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot utilization versus input job rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utilization_fn(logfile_path):\n",
    "    job_completion_times = []\n",
    "    with open(logfile_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-10000:]:\n",
    "            m = re.match(r'Cluster utilization: (\\d+\\.\\d+)', line)\n",
    "            if m is not None:\n",
    "                return float(m.group(1)) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V100s=25, P100s=0, K80s=0\")\n",
    "plot_metric_vs_inverse_lambda(\n",
    "    v100s=25, p100s=0, k80s=0,\n",
    "    policies=policies,\n",
    "    metric_fn=utilization_fn,\n",
    "    metric_label=\"Utilization (\\%)\",\n",
    "    ymax=120.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V100s=12, P100s=12, K80s=0\")\n",
    "plot_metric_vs_inverse_lambda(\n",
    "    v100s=12, p100s=12, k80s=0,\n",
    "    policies=policies,\n",
    "    metric_fn=utilization_fn,\n",
    "    metric_label=\"Utilization (\\%)\",\n",
    "    ymax=120.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V100s=16, P100s=8, K80s=0\")\n",
    "plot_metric_vs_inverse_lambda(\n",
    "    v100s=16, p100s=8, k80s=0,\n",
    "    policies=policies,\n",
    "    metric_fn=utilization_fn,\n",
    "    metric_label=\"Utilization (\\%)\",\n",
    "    ymax=120.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V100s=8, P100s=8, K80s=8\")\n",
    "plot_metric_vs_inverse_lambda(\n",
    "    v100s=8, p100s=8, k80s=8,\n",
    "    policies=policies,\n",
    "    metric_fn=utilization_fn,\n",
    "    metric_label=\"Utilization (\\%)\",\n",
    "    ymax=120.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot per-worker timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events(logfile_path):\n",
    "    events = {}\n",
    "    utilization = None\n",
    "    with open(logfile_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            m = re.match(r'(\\d+\\.\\d+).*scheduled.*Job ID: (\\d+)\\t'\n",
    "                          'Worker type: (.*)\\tWorker ID\\(s\\): ([\\d+\\,]*\\d+).*',\n",
    "                         line)\n",
    "            if m is not None:\n",
    "                start_timestamp = float(m.group(1))\n",
    "                job_id = int(m.group(2))\n",
    "                worker_type = m.group(3)\n",
    "                worker_ids = [int(x) for x in m.group(4).split(',')]\n",
    "                for worker_id in worker_ids:\n",
    "                    if worker_id not in events:\n",
    "                        events[worker_id] = []\n",
    "                    events[worker_id].append([job_id, worker_type,\n",
    "                                              start_timestamp,\n",
    "                                              None])\n",
    "            m = re.match(r'(\\d+\\.\\d+).*scheduled.*Job ID: \\((\\d+), (\\d+)\\)\\t'\n",
    "                          'Worker type: (.*)\\tWorker ID\\(s\\): ([\\d+\\,]*\\d+).*',\n",
    "                         line)\n",
    "            if m is not None:\n",
    "                start_timestamp = float(m.group(1))\n",
    "                job_id1 = int(m.group(2))\n",
    "                job_id2 = int(m.group(3))\n",
    "                worker_type = m.group(4)\n",
    "                worker_ids = [int(x) for x in m.group(5).split(',')]\n",
    "                for worker_id in worker_ids:\n",
    "                    if worker_id not in events:\n",
    "                        events[worker_id] = []\n",
    "                    events[worker_id].append([(job_id1, job_id2), worker_type,\n",
    "                                              start_timestamp,\n",
    "                                              None])\n",
    "            \n",
    "            m = re.match(r'(\\d+\\.\\d+).*succeeded.*Job ID: (\\d+)\\t'\n",
    "                          'Worker type: (.*)\\tWorker ID: (\\d+).*',\n",
    "                         line)\n",
    "            if m is not None:\n",
    "                end_timestamp = float(m.group(1))\n",
    "                job_id = int(m.group(2))\n",
    "                worker_type = m.group(3)\n",
    "                worker_id = int(m.group(4))\n",
    "                if worker_id not in events:\n",
    "                    continue\n",
    "                assert(events[worker_id][-1][0] == job_id and\n",
    "                       events[worker_id][-1][1] == worker_type and\n",
    "                       events[worker_id][-1][3] is None)\n",
    "                events[worker_id][-1][3] = end_timestamp\n",
    "                \n",
    "            m = re.match(r'(\\d+\\.\\d+).*succeeded.*Job ID: \\((\\d+), (\\d+)\\)\\t'\n",
    "                          'Worker type: (.*)\\tWorker ID: (\\d+).*',\n",
    "                         line)\n",
    "            if m is not None:\n",
    "                end_timestamp = float(m.group(1))\n",
    "                job_id1 = int(m.group(2))\n",
    "                job_id2 = int(m.group(3))\n",
    "                worker_type = m.group(4)\n",
    "                worker_id = int(m.group(5))\n",
    "                if worker_id not in events:\n",
    "                    continue\n",
    "                assert(events[worker_id][-1][0] == (job_id1, job_id2) and\n",
    "                       events[worker_id][-1][1] == worker_type and\n",
    "                       events[worker_id][-1][3] is None)\n",
    "                events[worker_id][-1][3] = end_timestamp\n",
    "                \n",
    "            m = re.match(r'Cluster utilization: (\\d+\\.\\d+)', line)\n",
    "            if m is not None:\n",
    "                utilization = float(m.group(1)) * 100.\n",
    "\n",
    "    return events, utilization\n",
    "\n",
    "def plot_timeline(v100s, p100s, k80s, policy, end, seed,\n",
    "                  output_filename=None):\n",
    "    relevant_logfile_paths = list(reversed(prune(\n",
    "        logfile_paths, v100s, p100s, k80s, policy, seed)))\n",
    "    for i, (lamb, relevant_logfile_path) in enumerate(relevant_logfile_paths):\n",
    "        if i % 2 != 0:\n",
    "            continue\n",
    "        events, utilization = get_events(relevant_logfile_path)\n",
    "\n",
    "        if utilization is not None:\n",
    "            plt.figure(figsize=(25, 6))\n",
    "            ax = plt.subplot2grid((1, 1), (0, 0), colspan=1)\n",
    "            print(\"Lambda = %.2f\" % lamb)\n",
    "\n",
    "            worker_id_to_type_mapping = {}\n",
    "            for worker_id in events:\n",
    "                for (job_id, worker_type, start_timestamp, end_timestamp) \\\n",
    "                    in events[worker_id]:\n",
    "                    if worker_id not in worker_id_to_type_mapping:\n",
    "                        worker_id_to_type_mapping[worker_id] = worker_type\n",
    "                    if start_timestamp < end:\n",
    "                        if isinstance(job_id, tuple):\n",
    "                            ax.plot([start_timestamp, min(end_timestamp, end)],\n",
    "                                    [worker_id, worker_id],\n",
    "                                    linewidth=10, c=\"yellow\")\n",
    "                        else:\n",
    "                            ax.plot([start_timestamp, min(end_timestamp, end)],\n",
    "                                    [worker_id, worker_id],\n",
    "                                    linewidth=10, c=\"C%d\" % (job_id % 10))\n",
    "            print(worker_id_to_type_mapping)\n",
    "            print(\"Utilization: %.3f%%\" % utilization)\n",
    "\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Worker ID\")\n",
    "        worker_ids = list(events.keys())\n",
    "        \n",
    "        ax.set_yticks(worker_ids)\n",
    "        ax.set_yticklabels([str(worker_id) for worker_id in worker_ids])\n",
    "        sns.despine()\n",
    "\n",
    "        if output_filename is not None:\n",
    "            with PdfPages(output_filename) as pdf:\n",
    "                pdf.savefig(bbox_inches='tight')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These timeline diagrams show the different micro-tasks that run on the different workers. Packed job combinations are in yellow, while single jobs are in other colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"V100s=25, P100s=0, K80s=0; Max-Min Fairness\")\n",
    "plot_timeline(\n",
    "    v100s=25, p100s=0, k80s=0, end=1000000,\n",
    "    policy=\"max_min_fairness\", seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"V100s=25, P100s=0, K80s=0; Max-Min Fairness with packing\")\n",
    "plot_timeline(\n",
    "    v100s=25, p100s=0, k80s=0, end=1000000,\n",
    "    policy=\"max_min_fairness_packed\", seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"V100s=25, P100s=0, K80s=0; FIFO\")\n",
    "plot_timeline(\n",
    "    v100s=25, p100s=0, k80s=0, end=1000000,\n",
    "    policy=\"fifo\", seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"V100s=25, P100s=0, K80s=0; Performance-aware FIFO\")\n",
    "plot_timeline(\n",
    "    v100s=25, p100s=0, k80s=0, end=1000000,\n",
    "    policy=\"fifo_perf\", seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V100s=8, P100s=8, K80s=8; FIFO\")\n",
    "plot_timeline(\n",
    "    v100s=8, p100s=8, k80s=8, end=1000000,\n",
    "    policy=\"fifo\", seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V100s=8, P100s=8, K80s=8; Performance-aware FIFO\")\n",
    "plot_timeline(\n",
    "    v100s=8, p100s=8, k80s=8, end=1000000,\n",
    "    policy=\"fifo_perf\", seed=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
