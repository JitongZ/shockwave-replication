{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-min fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we evaluate the impact of splitting larger optimization problems into smaller sub-problems on policy runtime and quality of solution for max-min fairness policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../../..\")\n",
    "from job_id_pair import JobIdPair\n",
    "import utils\n",
    "from plotting_utils import plot_runtimes, plot_effective_throughput_ratios\n",
    "from plotting_utils import plot_runtime_vs_effective_throughput_ratios\n",
    "from plotting_utils import plot_runtime_vs_effective_throughput_ratios_generic\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from partition_entities import split_generic, compute_precluster\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harness that runs a policy with the passed-in number of sub-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_sampler(a, n, max_val, min_offset):\n",
    "    arr = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        while True:\n",
    "            v = np.random.zipf(a,1) + min_offset\n",
    "            if v <= max_val:\n",
    "                break\n",
    "        arr[i] = v\n",
    "    return arr\n",
    "\n",
    "def create_problem_instance(num_jobs, cluster_spec,\n",
    "                            policy_name,\n",
    "                            seed,\n",
    "                            introduce_skew=False):\n",
    "    oracle_throughputs = utils.read_all_throughputs_json_v2(\"../../../simulation_throughputs.json\")\n",
    "    rng = random.Random()\n",
    "    rng.seed(seed)\n",
    "    jobs = {}\n",
    "    throughputs = {}\n",
    "    scale_factors = {}\n",
    "    priority_weights = {}\n",
    "    #heavy_job_ids = rng.sample(range(0,num_jobs), min(num_jobs,32))\n",
    "    weights = np.round(zipf_sampler(2.1, num_jobs, 512, 0))\n",
    "    for i in range(num_jobs):\n",
    "        job_id = JobIdPair(i, None)\n",
    "        job = utils.generate_job(throughputs=oracle_throughputs,\n",
    "                                 rng=rng, job_id=job_id)\n",
    "        jobs[job_id[0]] = job\n",
    "        job_type_key = (job.job_type, job.scale_factor)\n",
    "        throughputs[job_id] = {}\n",
    "        for worker_type in cluster_spec:\n",
    "            throughputs[job_id][worker_type] = \\\n",
    "                oracle_throughputs[worker_type][job_type_key]['null']\n",
    "        scale_factors[job_id] = 1\n",
    "        priority_weights[job_id] = 1.0\n",
    "        if introduce_skew:\n",
    "            scale_factors[job_id] = weights[i]\n",
    "            priority_weights[job_id] = weights[i]\n",
    "            #priority_weights[job_id] = (i % 4) + 1.0\n",
    "            #if (i in heavy_job_ids):\n",
    "                #print(\"created big job\")\n",
    "            #    scale_factors[job_id] = 512\n",
    "            #    priority_weights[job_id] = 512\n",
    "        \n",
    "    if 'pack' in policy_name:\n",
    "        for i in range(num_jobs):\n",
    "            job_type_key = (jobs[i].job_type, jobs[i].scale_factor)\n",
    "            for j in range(num_jobs):\n",
    "                if i < j and jobs[i].scale_factor == jobs[j].scale_factor:\n",
    "                    other_job_type_key = \\\n",
    "                        (jobs[j].job_type, jobs[j].scale_factor)\n",
    "                    throughputs[JobIdPair(i, j)] = {}\n",
    "                    for worker_type in cluster_spec:\n",
    "                        throughputs[JobIdPair(i, j)][worker_type] = \\\n",
    "                            oracle_throughputs[worker_type][job_type_key][other_job_type_key]\n",
    "    return throughputs, scale_factors, priority_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harness(policy, throughputs, scale_factors, priority_weights, cluster_spec, num_sub_clusters=1,\n",
    "            random_cluster_assignment=False, split_generic=False, split_method='means'):\n",
    "    start_time = time.time()\n",
    "    sub_cluster_throughputs = []\n",
    "    sub_cluster_scale_factors = []\n",
    "    sub_cluster_priority_weights = []\n",
    "    job_to_sub_cluster_assignment = {}\n",
    "    job_ids = []\n",
    "    for job_id in throughputs:\n",
    "        if not job_id.is_pair():\n",
    "            job_ids.append(job_id)\n",
    "    ##        \n",
    "    if split_generic:\n",
    "        job_to_sub_cluster_assignment = split_generic_wrapper(job_ids, throughputs, scale_factors, \n",
    "                                                              priority_weights, num_sub_clusters,\n",
    "                                                              method=split_method,\n",
    "                                                              verbose=False)\n",
    "    else:\n",
    "    ##    \n",
    "        for i, job_id in enumerate(job_ids):\n",
    "            if random_cluster_assignment:\n",
    "                job_to_sub_cluster_assignment[job_id[0]] = random.randint(0, num_sub_clusters-1)\n",
    "            else:\n",
    "                job_to_sub_cluster_assignment[job_id[0]] = job_id[0] % num_sub_clusters\n",
    "    for i in range(num_sub_clusters):\n",
    "        sub_cluster_throughputs.append({})\n",
    "        sub_cluster_scale_factors.append({})\n",
    "        sub_cluster_priority_weights.append({})\n",
    "        for job_id in throughputs:\n",
    "            \n",
    "            if (job_to_sub_cluster_assignment[job_id[0]] == i) and (\n",
    "                 not job_id.is_pair() or (job_to_sub_cluster_assignment[job_id[1]] == i)):\n",
    "                sub_cluster_throughputs[-1][job_id] = copy.copy(throughputs[job_id])\n",
    "                if not job_id.is_pair():\n",
    "                    sub_cluster_scale_factors[-1][job_id] = scale_factors[job_id]\n",
    "                    sub_cluster_priority_weights[-1][job_id] = priority_weights[job_id]\n",
    "    sub_cluster_cluster_spec = {worker_type: cluster_spec[worker_type] // num_sub_clusters\n",
    "                                for worker_type in cluster_spec}\n",
    "    setup_time = time.time() - start_time\n",
    "    full_allocation = {}\n",
    "    computation_times = []\n",
    "    for i in range(num_sub_clusters):\n",
    "        start_time = time.time()\n",
    "        if policy._name.startswith('MaxMinFairness'):\n",
    "            sub_cluster_allocation = policy.get_allocation(\n",
    "                sub_cluster_throughputs[i], sub_cluster_scale_factors[i],\n",
    "                sub_cluster_priority_weights[i], sub_cluster_cluster_spec)\n",
    "        else:\n",
    "            sub_cluster_allocation = policy.get_allocation(\n",
    "                sub_cluster_throughputs[i], sub_cluster_scale_factors[i],\n",
    "                sub_cluster_cluster_spec)\n",
    "        for job_id in sub_cluster_allocation:\n",
    "            full_allocation[job_id] = sub_cluster_allocation[job_id]\n",
    "        computation_times.append(time.time() - start_time - setup_time)\n",
    "    return full_allocation, setup_time + max(computation_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep(policy_names_and_num_sub_clusters,\n",
    "          all_num_jobs,\n",
    "          num_trials, introduce_skew=False,\n",
    "          split_generic=False,\n",
    "          split_method='means'):\n",
    "    all_runtimes = {}\n",
    "    all_effective_throughputs = {}\n",
    "    all_high_effective_throughputs = {}\n",
    "    all_low_effective_throughputs = {}\n",
    "    for num_jobs in all_num_jobs:\n",
    "        all_runtimes[num_jobs] = []\n",
    "        all_effective_throughputs[num_jobs] = []\n",
    "        all_high_effective_throughputs[num_jobs] = []\n",
    "        all_low_effective_throughputs[num_jobs] = []\n",
    "        cluster_spec = {\n",
    "            'v100': max(num_jobs // 4, 1),\n",
    "            'p100': max(num_jobs // 4, 1),\n",
    "            'k80': max(num_jobs // 4, 1),\n",
    "        }\n",
    "        for i in range(num_trials):\n",
    "            throughputs, scale_factors, priority_weights = \\\n",
    "                create_problem_instance(num_jobs, cluster_spec,\n",
    "                                        policy_names_and_num_sub_clusters[0][0], seed=i,\n",
    "                                        introduce_skew=introduce_skew)\n",
    "            all_runtimes[num_jobs].append([])\n",
    "            allocations = []\n",
    "            for (policy_name, num_sub_clusters) in policy_names_and_num_sub_clusters:\n",
    "                policy = utils.get_policy(policy_name, solver='ECOS')\n",
    "                allocation, runtime = harness(\n",
    "                    policy, throughputs,\n",
    "                    scale_factors,\n",
    "                    priority_weights,\n",
    "                    cluster_spec,\n",
    "                    num_sub_clusters=num_sub_clusters,\n",
    "                    split_generic=split_generic,\n",
    "                    split_method=split_method)\n",
    "                all_runtimes[num_jobs][-1].append(runtime)\n",
    "                allocations.append(allocation)\n",
    "\n",
    "            all_effective_throughputs[num_jobs].append([])\n",
    "            all_high_effective_throughputs[num_jobs].append([])\n",
    "            all_low_effective_throughputs[num_jobs].append([])\n",
    "            for allocation in allocations:\n",
    "                effective_throughputs = {}\n",
    "                high_effective_throughputs = {}\n",
    "                low_effective_throughputs = {}\n",
    "                for job_id in allocation:\n",
    "                    for single_job_id in job_id.singletons():\n",
    "                        if single_job_id not in effective_throughputs:\n",
    "                            effective_throughputs[single_job_id] = 0.0\n",
    "                            if priority_weights[single_job_id] > 100.0:\n",
    "                                high_effective_throughputs[single_job_id] = 0.0\n",
    "                            else:\n",
    "                                low_effective_throughputs[single_job_id] = 0.0\n",
    "                    for worker_type in allocation[job_id]:\n",
    "                        if job_id.is_pair():\n",
    "                            for i, single_job_id in enumerate(job_id.singletons()):\n",
    "                                effective_throughputs[single_job_id] += (\n",
    "                                   allocation[job_id][worker_type] *\n",
    "                                   throughputs[job_id][worker_type][i]\n",
    "                                )\n",
    "                        else:\n",
    "                            et_add = (allocation[job_id][worker_type] *\n",
    "                                      throughputs[job_id][worker_type])\n",
    "                            effective_throughputs[job_id] += et_add\n",
    "                            if priority_weights[single_job_id] > 100.0:\n",
    "                                high_effective_throughputs[job_id] += et_add\n",
    "                            else:\n",
    "                                low_effective_throughputs[job_id] += et_add\n",
    "                all_effective_throughputs[num_jobs][-1].append(effective_throughputs)\n",
    "                all_high_effective_throughputs[num_jobs][-1].append(high_effective_throughputs)\n",
    "                all_low_effective_throughputs[num_jobs][-1].append(low_effective_throughputs)\n",
    "    return all_runtimes, all_effective_throughputs, \\\n",
    "           all_high_effective_throughputs, all_low_effective_throughputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effective throughputs and runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runtimes_and_effective_throughputs(policy_name, all_num_sub_clusters,\n",
    "                                           num_jobs, introduce_skew=False,\n",
    "                                           split_generic=False, split_method='means'):\n",
    "    policy_names_and_num_sub_clusters = [\n",
    "        (policy_name, num_sub_clusters)\n",
    "        for num_sub_clusters in all_num_sub_clusters\n",
    "    ]\n",
    "    #if not introduce_skew:\n",
    "    #    policy_names_and_num_sub_clusters.append(\n",
    "    #        ('gandiva', 1))\n",
    "    all_runtimes, all_effective_throughputs, \\\n",
    "    all_high_effective_throughputs, all_low_effective_throughputs = sweep(\n",
    "        policy_names_and_num_sub_clusters, [num_jobs],\n",
    "        num_trials=1, introduce_skew=introduce_skew,\n",
    "        split_generic=split_generic, split_method=split_method)\n",
    "    runtimes = all_runtimes[num_jobs][0]\n",
    "    all_effective_throughputs = all_effective_throughputs[num_jobs][0]\n",
    "    high_effective_throughputs = all_high_effective_throughputs[num_jobs][0]\n",
    "    low_effective_throughputs = all_low_effective_throughputs[num_jobs][0]\n",
    "    return runtimes, all_effective_throughputs, \\\n",
    "            high_effective_throughputs, low_effective_throughputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runtimes(policy_name, all_num_jobs, introduce_skew=False):\n",
    "    data = {\n",
    "        'policy': [],\n",
    "        'num_jobs': [],\n",
    "        'runtimes': []\n",
    "    }\n",
    "    all_num_sub_clusters = [1, 4, 16]\n",
    "    policy_names_and_num_sub_clusters = [\n",
    "        (policy_name, num_sub_clusters)\n",
    "        for num_sub_clusters in all_num_sub_clusters\n",
    "    ]\n",
    "    policy_labels = ['Vanilla', '4 sub-clusters', '16 sub-clusters']\n",
    "    all_runtimes, _ = sweep(policy_names_and_num_sub_clusters,\n",
    "                            all_num_jobs,\n",
    "                            num_trials=1,\n",
    "                            introduce_skew=introduce_skew)\n",
    "    for num_jobs in all_runtimes:\n",
    "        for i in range(len(all_runtimes[num_jobs])):\n",
    "            for (policy_label, runtime) in zip(\n",
    "                policy_labels, all_runtimes[num_jobs][i]):\n",
    "                data['policy'].append(policy_label)\n",
    "                data['num_jobs'].append(num_jobs)\n",
    "                data['runtimes'].append(runtime)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effective throughput ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effective_throughput_ratios(policy_name, num_jobs, introduce_skew=False,\n",
    "                                    random_cluster_assignment=False):\n",
    "    random.seed(42)\n",
    "    all_num_sub_clusters = [1, 4, 16]\n",
    "    policy_names_and_num_sub_clusters = [\n",
    "        (policy_name, num_sub_clusters)\n",
    "        for num_sub_clusters in all_num_sub_clusters\n",
    "    ]\n",
    "    _, all_effective_throughputs = sweep(\n",
    "        policy_names_and_num_sub_clusters,\n",
    "        [num_jobs],\n",
    "        num_trials=1,\n",
    "        introduce_skew=introduce_skew)\n",
    "    return all_effective_throughputs[num_jobs][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function for calling split_generic, return job to subcluster mapping\n",
    "def split_generic_wrapper(job_ids_list, throughputs, scale_factors, priority_weights, num_subproblems, \n",
    "                          method='means', verbose=False):\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"job ids: \" + str(job_ids_list))\n",
    "        print(\"throughputs: \" + str(throughputs))\n",
    "        print(\"scale factors: \" + str(scale_factors))\n",
    "        print(\"priority weights: \" + str(priority_weights))\n",
    "    \n",
    "    job_to_subcluster_assignment = {}\n",
    "    \n",
    "    # if theres only 1 subproblem, assign all entities to it and return\n",
    "    if num_subproblems == 1:\n",
    "        for job_id in job_ids_list:\n",
    "            job_to_subcluster_assignment[job_id[0]] = 0\n",
    "        return job_to_subcluster_assignment\n",
    "            \n",
    "    all_inputs_dict = {}\n",
    "    num_resources = len(list(throughputs.values())[0])\n",
    "    num_jobs = len(throughputs)\n",
    "    np_data = np.zeros((num_jobs,num_resources+2))# dimensions are each resourse, scale factor, and priority\n",
    "    \n",
    "    # get mean of each input dimension\n",
    "    scale_factors_mean = np.mean([item for item in scale_factors.values()])\n",
    "    priority_mean = np.mean([item for item in priority_weights.values()])\n",
    "    throughput_means = [sum([list(t.values())[i] for t in throughputs.values()])/num_jobs for i in range(num_resources)]\n",
    "    \n",
    "    \n",
    "    # create dict mapping job id to list of features (normalized by mean of said feature)\n",
    "    for i, job_id in enumerate(job_ids_list):\n",
    "        throughput_dict = throughputs[job_id]\n",
    "        job_input_list = [list(throughput_dict.values())[i]/throughput_means[i] for i in range(len(throughput_dict))]\n",
    "        \n",
    "        job_input_list.append(scale_factors[job_id]/scale_factors_mean)\n",
    "        job_input_list.append(priority_weights[job_id]/priority_mean)\n",
    "        all_inputs_dict[job_id[0]] = job_input_list\n",
    "        np_data[i,:] = job_input_list\n",
    "        \n",
    "    # assign jobs to different subproblems, then create assignment dict\n",
    "    precluster = None\n",
    "    if method is 'cluster':\n",
    "        precluster = compute_precluster(np_data, 27)\n",
    "    \n",
    "    subproblem_assignments = split_generic(all_inputs_dict, np_data, num_subproblems, \n",
    "                                           method=method, precluster=precluster)\n",
    "    if method is 'cluster':\n",
    "        # replace job id indices with job ids\n",
    "        for subproblem_assignment in subproblem_assignments:\n",
    "            subproblem_assignment = [job_ids_list[x] for x in subproblem_assignment]\n",
    "            \n",
    "    for sa in subproblem_assignments:\n",
    "        print(len(sa))\n",
    "    \n",
    "    for i, subproblem_list in enumerate(subproblem_assignments):\n",
    "        for job in subproblem_list:\n",
    "            job_to_subcluster_assignment[job] = i\n",
    "            \n",
    "    return job_to_subcluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run skewed tests with random, two-choice mean matching, and clustering splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = ['Original', 'POP-2', 'POP-4', 'POP-8', 'POP-16', 'POP-32']#, 'POP-64']#,'Gandiva']\n",
    "all_num_sub_clusters = [1, 2, 4, 8, 16, 32]#, 64]\n",
    "num_jobs = 2**14\n",
    "runtimes_rand, all_effective_throughputs_rand,\\\n",
    "high_effective_throughputs_rand, low_effective_throughputs_rand = \\\n",
    "    get_runtimes_and_effective_throughputs('max_min_fairness',\n",
    "                                           all_num_sub_clusters,\n",
    "                                           num_jobs,\n",
    "                                           introduce_skew=True,\n",
    "                                           split_generic=False,\n",
    "                                           split_method='means')\n",
    "#plot_runtime_vs_effective_throughput_ratios_generic(\n",
    "#    runtimes, all_effective_throughputs, labels,\n",
    "#    output_filename=\"effective_throughput_ratio_and_runtimes/num_jobs=\"+str(num_jobs)+\".pdf\")\n",
    "#plot_runtime_vs_effective_throughput_ratios_generic(\n",
    "#    runtimes, high_effective_throughputs, labels)\n",
    "#plot_runtime_vs_effective_throughput_ratios_generic(\n",
    "#    runtimes, low_effective_throughputs, labels)\n",
    "\n",
    "runtimes_means, all_effective_throughputs_means,\\\n",
    "high_effective_throughputs_means, low_effective_throughputs_means = \\\n",
    "    get_runtimes_and_effective_throughputs('max_min_fairness',\n",
    "                                           all_num_sub_clusters,\n",
    "                                           num_jobs,\n",
    "                                           introduce_skew=True,\n",
    "                                           split_generic=True,\n",
    "                                           split_method='means')\n",
    "#plot_runtime_vs_effective_throughput_ratios_generic(\n",
    "#    runtimes, all_effective_throughputs, labels,\n",
    "#    output_filename=\"effective_throughput_ratio_and_runtimes/num_jobs=\"+str(num_jobs)+\".pdf\")\n",
    "#plot_runtime_vs_effective_throughput_ratios_generic(\n",
    "#    runtimes, high_effective_throughputs, labels)\n",
    "#plot_runtime_vs_effective_throughput_ratios_generic(\n",
    "#    runtimes, low_effective_throughputs, labels)\n",
    "\n",
    "runtimes_clust, all_effective_throughputs_clust,\\\n",
    "high_effective_throughputs_clust, low_effective_throughputs_clust = \\\n",
    "    get_runtimes_and_effective_throughputs('max_min_fairness',\n",
    "                                           all_num_sub_clusters,\n",
    "                                           num_jobs,\n",
    "                                           introduce_skew=True,\n",
    "                                           split_generic=True,\n",
    "                                           split_method='cluster')\n",
    "#plot_runtime_vs_effective_throughput_ratios_generic(\n",
    "#    runtimes, all_effective_throughputs, labels,\n",
    "#    output_filename=\"effective_throughput_ratio_and_runtimes/num_jobs=\"+str(num_jobs)+\".pdf\")\n",
    "#plot_runtime_vs_effective_throughput_ratios_generic(\n",
    "#    runtimes, high_effective_throughputs, labels)\n",
    "#plot_runtime_vs_effective_throughput_ratios_generic(\n",
    "#    runtimes, low_effective_throughputs, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_mean(all_effective_throughputs):\n",
    "    job_ids = list(all_effective_throughputs[0].keys())\n",
    "    means = []\n",
    "    for effective_throughputs in all_effective_throughputs:\n",
    "        effective_throughput_ratios = np.array([\n",
    "            effective_throughputs[job_id] / all_effective_throughputs[0][job_id]\n",
    "            for job_id in job_ids])\n",
    "        mean_t = np.mean(effective_throughput_ratios)\n",
    "        means.append(mean_t)\n",
    "    return means\n",
    "\n",
    "plt.figure(figsize=(6.5, 3))\n",
    "ax = plt.subplot2grid((1, 1), (0, 0), colspan=1)\n",
    "ax.plot(all_num_sub_clusters, comp_mean(all_effective_throughputs_rand), 'r', marker='*', label='random')\n",
    "ax.plot(all_num_sub_clusters, comp_mean(all_effective_throughputs_means), 'g', marker='*', label='means')\n",
    "ax.plot(all_num_sub_clusters, comp_mean(all_effective_throughputs_clust), 'b', marker='*', label='cluster')\n",
    "ax.set_ylabel(\"Throughput ratio\")\n",
    "ax.set_xlabel(\"K\")\n",
    "ax.legend()\n",
    "plt.title(\"all jobs\")\n",
    "\n",
    "plt.figure(figsize=(6.5, 3))\n",
    "ax = plt.subplot2grid((1, 1), (0, 0), colspan=1)\n",
    "ax.plot(all_num_sub_clusters, comp_mean(high_effective_throughputs_rand), 'r', marker='*', label='random')\n",
    "ax.plot(all_num_sub_clusters, comp_mean(high_effective_throughputs_means), 'g', marker='*', label='means')\n",
    "ax.plot(all_num_sub_clusters, comp_mean(high_effective_throughputs_clust), 'b', marker='*', label='cluster')\n",
    "ax.set_ylabel(\"Throughput ratio\")\n",
    "ax.set_xlabel(\"K\")\n",
    "ax.legend()\n",
    "plt.title(\"heavy jobs\")\n",
    "\n",
    "plt.figure(figsize=(6.5, 3))\n",
    "ax = plt.subplot2grid((1, 1), (0, 0), colspan=1)\n",
    "ax.plot(all_num_sub_clusters, comp_mean(low_effective_throughputs_rand), 'r', marker='*', label='random')\n",
    "ax.plot(all_num_sub_clusters, comp_mean(low_effective_throughputs_means), 'g', marker='*', label='means')\n",
    "ax.plot(all_num_sub_clusters, comp_mean(low_effective_throughputs_clust), 'b', marker='*', label='cluster')\n",
    "ax.set_ylabel(\"Throughput ratio\")\n",
    "ax.set_xlabel(\"K\")\n",
    "ax.legend()\n",
    "plt.title(\"light jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play around with zipf distribution parameter to see how many heavy jobs we get\n",
    "#weights = np.random.zipf(1.2, size=2000)\n",
    "weights = np.floor(zipf_sampler(2.1, 2**14, 512, 0))\n",
    "print(np.sum(weights))\n",
    "print(min(weights))\n",
    "plt.hist(weights, bins=50)\n",
    "plt.show()\n",
    "len(weights[weights > 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
